{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49085af9-366c-48cb-9d92-77a1d854fd8d",
   "metadata": {},
   "source": [
    "## Generador de acrónimos \n",
    "\n",
    "Este ejemplo de jupyter notebook permite generar acrónimos a partir de una cadena de texto proporcionada como entrada y un corpus de palabras.\n",
    "\n",
    "El algoritmo devuelve palabras que están contenidas en el título, de forma que al menos una letra pertenece a cada palabra, siempre y\n",
    "cuando la palabra no sea un stopword (por ejemplo, \"of\"), en cuyo caso es opcional incluir o no una de sus letras en el acrónimo.\n",
    "\n",
    "Por defecto, no distingue minúsculas y mayúsculas. La distinción entre mayúsculas y minúsculas implica que solo las letras mayúsculas son consideradas para el alineamiento.\n",
    "\n",
    "Así, por ejemplo si tenemos en cuenta las mayúsculas y proporcionamos la cadena de texto \"Platform for OPEN DATA ACCESS IN DIGITAL HUMANITIES RESEARCH\", el proceso fuerza \n",
    "fuerza a que el acrónimo empiece por P y a que ningún término \"for\" sea incluido. En cambio, pueden aparecer o no letras de \"IN\" por ser un stopword.\n",
    "\n",
    "Si no tenemos en cuenta la distinción entre mayúsculas y minúsculas, en realidad todo el título se transforma a mayúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "839831ab-559d-4b3d-8378-61bf345e5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019a525-f03f-499d-bc2e-b6827194d2cc",
   "metadata": {},
   "source": [
    "## Clase Vocabulary para almacenar el diccionario proporcionado como entrada.\n",
    "\n",
    "Leemos línea a línea el fichero que recibimos como parámetro. Cada línea incluye una palabra a la que le quitamos los espacios de la izquierda y derecha y convertimos a mayúscula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af5da598-097e-4404-9bfc-5fbc178e6543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un conjunto de nombres (palabras en mayúsculas) que son acrónimos válidos\n",
    "class Vocabulary(set):\n",
    "    def __init__(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            for w in f: # lee linea a linea\n",
    "                self.add(w.strip().upper()) # eliminamos espacios y convertimos a mayusculas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891e121-f3ac-42e6-bde9-adc12a8308f6",
   "metadata": {},
   "source": [
    "## ¿Cómo se utiliza la clase Vocabulary?\n",
    "Para utilizar la clase Vocabulary es necesario el siguiente código que lee el fichero en_words.txt de la carpeta input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cd7c76f-aea4-45be-8802-767d377f6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Vocabulary('input/en_words.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9470ec8-ba19-45bc-a02d-a6dec84b8a9b",
   "metadata": {},
   "source": [
    "## La clase Title \n",
    "\n",
    "La clase Title almacena una cadena de texto que incluye varios términos. Contemplamos el caso de distinción entre mayúsculas y minúsculas.\n",
    "\n",
    "En el caso de tener en cuenta las mayúsculas, únicamente los términos en mayúscula se tendrán en cuenta. Por ejemplo, en el caso de utilizar la cadena \"First PLAN\", solo se tendrá en cuenta la F del primer término y P, L, A, o N del segundo.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92d867bd-7ae0-4ce0-9f32-68c0c0a80740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Title(str):\n",
    "    def __new__(cls, content, case_sensitive=True):\n",
    "        \n",
    "        # reducimos posibles espacios consecutivos a uno solo\n",
    "        reduced = ' '.join(content.split())\n",
    "        \n",
    "        # contemplamos la distinción de mayúsculas y minúsculas \n",
    "        if case_sensitive:\n",
    "            return super().__new__(cls, reduced)\n",
    "            str.__init__(reduced)\n",
    "        else:\n",
    "            return super().__new__(cls, reduced.upper())\n",
    "    \n",
    "    # En el caso de tener en cuenta las mayúsculas, únicamente los términos en mayúscula se tendrán en cuenta. \n",
    "    # Por ejemplo, en el caso de utilizar la cadena \"First PLAN\", solo se tendrá en cuenta la F del primer término y P, L, A, o N del segundo.\n",
    "    def __init__(self, content, case_sensitive=True):\n",
    "        self._tokens = self.split()\n",
    "\n",
    "        # marcamos la posición de los términos dependiendo del número de espacios precedentes\n",
    "        self._token_number = {n:self[:n].count(' ') for n in range(len(self)) if self[n] != ' '}\n",
    "\n",
    "    \n",
    "    # devuelve la lista de términos en el titulo\n",
    "    def tokens(self):\n",
    "        return self._tokens\n",
    "    \n",
    "    # devuelve la posición del término en el titulo\n",
    "    #return the token number for the specified position in the title\n",
    "    def token_number(self, pos):\n",
    "        return self._token_number[pos]\n",
    "    \n",
    "    # Devuelve verdadero si el término es una subsecuencia del titulo,\n",
    "    # el término es el resultado de eliminar algunos caracteres (o ninguno) en el titulo\n",
    "    def contains(self, word):\n",
    "        n = -1\n",
    "        for c in word:\n",
    "            n = self.find(c, n + 1) \n",
    "            if n < 0:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    # Devuelve todos los posibles alineamientos de la palabra y el titulo.\n",
    "    # Un alineamiento es una tupla a = (a1, a2, .., aN) donde a1 < a2 < .. < aN y word[k] = title[ak] \n",
    "    # Por ejemplo, si el término es 'AB' y el texto es 'ABAB', los alineamientos seran (0, 1), (0, 3) y (2,3)\n",
    "    def all_alignments(self, word):\n",
    "        A = [list(), list()]\n",
    "        wsize = len(word)\n",
    "        tsize = len(self)\n",
    "        for j in range(1 + tsize):\n",
    "            A[0].append(set())\n",
    "        \n",
    "        for i in range(1, 1 + wsize):\n",
    "            A[i % 2] = [set()]\n",
    "            for j in range(1, 1 + tsize):\n",
    "                a = A[i%2][j - 1].copy()\n",
    "                A[i % 2].append(a)\n",
    "                if word[i - 1] == self[j - 1]: # add j - 1 to the tuples\n",
    "                    A[i % 2][j].add((j - 1,)) \n",
    "                    for a in A[(i - 1) % 2][j - 1]:\n",
    "                        A[i % 2][j].add(a + (j - 1,))\n",
    "            \n",
    "        # return only full aligments (word is exhausted and all chars matched)\n",
    "        return set(a for a in A[wsize % 2][-1] if len(a) == len(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b0dad-fe9b-4ac4-8aa1-d2b1a517b826",
   "metadata": {},
   "source": [
    "## Vamos a probar el generador de acrónimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5442b703-5219-44df-bbde-2e948955116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform for OPEN DATA ACCESS in DIGITAL HUMANITIES RESEARCH\n",
      "lexicon has 370103 words\n",
      "PEACELESS :  Platform for opEn datA aCcEss in digitaL humanitiES reSearch\n",
      "PEASANTESS :  Platform for opEn datA acceSs in digitAl humaNiTiES reSearch\n",
      "PEASANTS :  Platform for opEn datA acceSs in digitAl humaNiTies reSearch\n",
      "PEDAGESE :  Platform for opEn Data Access in diGital humanitiES resEarch\n",
      "PEDALER :  Platform for opEn Data Access in digitaL humanitiEs Research\n",
      "PEDALIER :  Platform for opEn Data Access in digitaL humanItiEs reseaRch\n",
      "PEDALIERS :  Platform for opEn Data Access in digitaL humanItiEs ReSearch\n",
      "PEDALITER :  Platform for opEn Data Access in digitaL humanITiEs reseaRch\n",
      "PEDETES :  Platform for opEn Data accEss in digiTal humanitiEs reSearch\n",
      "PEDETIC :  Platform for opEn Data accEss in digiTal humanitIes researCh\n",
      "PENTACETATE :  Platform for opEN daTa AcCEss in digiTal humAniTies rEsearch\n",
      "PENTACLES :  Platform for opEN daTA acCess in digitaL humanitiEs reSearch\n",
      "PENTADIC :  Platform for opEN daTa Access in Digital humanitIes researCh\n",
      "PENTASTICH :  Platform for opEN daTa AcceSs in digiTal humanitIes researCH\n",
      "PENTELIC :  Platform for opEN daTa accEss in digitaL humanitIes researCh\n",
      "PETALIA :  Platform for opEn daTa Access in digitaL humanIties reseArch\n",
      "PETALITE :  Platform for opEn daTa Access in digitaL humanITies resEarch\n",
      "PETASITES :  Platform for opEn daTA acceSs in dIgiTal humanitiEs reSearch\n",
      "PODALIC :  Platform for Open Data Access in digitaL humanitIes researCh\n",
      "PODATUS :  Platform for Open Data Access in digiTal hUmanities reSearch\n",
      "PODESTAS :  Platform for Open Data accESs in digiTal humAnities reSearch\n",
      "PODETIA :  Platform for Open Data accEss in digiTal humanitIes reseArch\n",
      "POETASTER :  Platform for OpEn daTA acceSs in digiTal humanitiEs Research\n",
      "POETASTERS :  Platform for OpEn daTa AcceSs in digiTal humanitiEs ReSearch\n",
      "POTAGER :  Platform for Open daTa Access in diGital humanitiEs reseaRch\n",
      "POTAGERE :  Platform for Open daTa Access in diGital humanitiEs REsearch\n",
      "POTAGES :  Platform for Open daTa Access in diGital humanitiEs reSearch\n",
      "POTESTAS :  Platform for Open daTa accEsS in digiTal humAnities reSearch\n",
      "POTESTATE :  Platform for Open daTa accEsS in digiTal humAniTies resEarch\n",
      "POTSIES :  Platform for Open daTa accesS in digItal humanitiEs reSearch\n"
     ]
    }
   ],
   "source": [
    "case_sensitive = True\n",
    "\n",
    "title = Titulo('Platform for OPEN DATA ACCESS in DIGITAL HUMANITIES RESEARCH', case_sensitive)\n",
    "print(titulo)\n",
    "    \n",
    "words = Vocabulary('input/en_words.txt')\n",
    "print(\"lexicon has\", len(words), 'words')\n",
    "\n",
    "# los stopwords se pueden tener en cuenta opcionalmente\n",
    "stopwords = Vocabulary('input/en_stopwords.txt')\n",
    "lowercase = {token for token in title.tokens() if token.islower()}\n",
    "\n",
    "# terminos en el titulo que no es necesario tener en cuenta\n",
    "ignore = {n for n, token in enumerate(title.tokens()) if token.upper() in stopwords|lowercase}\n",
    "\n",
    "for word in sorted(words):\n",
    "    if title.contains(word):\n",
    "        alignments = title.all_alignments(word)\n",
    "        for a in alignments:\n",
    "            matched = {title.token_number(pos) for pos in a}\n",
    "            if len(ignore | matched) == len(title.tokens()):\n",
    "                res = [c.upper() if n in a else c.lower() for n, c in enumerate(title)]\n",
    "                print(word, ': ', ''.join(res))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d95079-37fc-4773-995b-f6ae2b0bf145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
